{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch pandas datasets transformers scikit-learn datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "import json\n",
    "from torch import cuda\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    BertForSequenceClassification, \n",
    "    BertTokenizerFast,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset from Hugging Face and using pandas to read the train and test datasets into dataframes, in order to analyse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meta-llama/Meta-Llama-3-8B-Instruct_valid.json') as f:\n",
    "    data = json.load(f)\n",
    "    for e in data:\n",
    "        del e['id']\n",
    "        e['text'] = e['Completion']\n",
    "        del e['Completion']\n",
    "        e['labels'] = 0 \n",
    "        e['category'] = 'Meta-Llama-3-8B-Instruct'\n",
    "df_llama = pd.DataFrame(data)\n",
    "\n",
    "with open('microsoft/Phi-3-mini-4k-instruct_valid.json') as f:\n",
    "    data = json.load(f)\n",
    "    for e in data:\n",
    "        del e['id']\n",
    "        e['text'] = e['Completion']\n",
    "        del e['Completion']\n",
    "        e['labels'] = 1\n",
    "        e['category'] = 'Phi-3-mini-4k-instruct'   \n",
    "df_phi3 = pd.DataFrame(data)\n",
    "\n",
    "with open('mistralai/Mixtral-8x7B-Instruct-v0.1_valid.json') as f:\n",
    "    data = json.load(f)\n",
    "    for e in data:\n",
    "        del e['id']\n",
    "        e['text'] = e['Completion']\n",
    "        del e['Completion']\n",
    "        e['labels'] = 2\n",
    "        e['category'] = 'Mixtral-8x7B-Instruct-v0.1'\n",
    "df_mixtral = pd.DataFrame(data)\n",
    "\n",
    "with open('openai/GPT4_valid.json') as f:\n",
    "    data = json.load(f)\n",
    "    for e in data:\n",
    "        del e['id']\n",
    "        e['text'] = e['Completion']\n",
    "        del e['Completion']\n",
    "        e['labels'] = 3\n",
    "        e['category'] = 'GPT4'\n",
    "df_gpt4 = pd.DataFrame(data)\n",
    "\n",
    "df_combined = pd.concat([df_llama, df_phi3, df_mixtral, df_gpt4], ignore_index=True)\n",
    "      \n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_combined = df_combined['labels'] \n",
    "\n",
    "\n",
    "label_counts_combined = labels_combined.value_counts()\n",
    "\n",
    "label_counts_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Meta-Llama-3-8B-Instruct','Phi-3-mini-4k-instruct', 'Mixtral-8x7B-Instruct-v0.1', 'GPT4']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = len(labels)\n",
    "\n",
    "id2label={id:label for id,label in enumerate(labels)}\n",
    "label2id={label:id for id,label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'labels'\n",
    "\n",
    "test_size = 0.8\n",
    "\n",
    "df_test_eval, df_train = train_test_split(df_combined, test_size=test_size, stratify=df_combined[target_variable], random_state=42)\n",
    "\n",
    "print(\"Train Set Shape:\", df_train.shape)\n",
    "print(\"Test and Evaluation Sets Shape:\", df_test_eval.shape)\n",
    "\n",
    "\n",
    "label_counts_train = df_train[target_variable].value_counts()\n",
    "label_counts_test_eval = df_test_eval[target_variable].value_counts()\n",
    "print(\"Label counts in train set:\\n\", label_counts_train)\n",
    "print(\"Label counts in test and evaluation sets:\\n\", label_counts_test_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['category'].value_counts().plot(kind='bar', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divided the test dataset into test and validation. Using train_test_split function we can control the proportion of data going to the validation set and that the test and evaluation sets have a similar class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'labels'\n",
    "\n",
    "test_size = 0.5\n",
    "\n",
    "df_test, df_eval = train_test_split(df_test_eval, test_size=test_size, stratify=df_test_eval[target_variable], random_state=42)\n",
    "\n",
    "print(\"Test Set Shape:\", df_test.shape)\n",
    "print(\"Evaluation Set Shape:\", df_eval.shape)\n",
    "\n",
    "\n",
    "label_counts_test = df_test[target_variable].value_counts()\n",
    "label_counts_eval = df_eval[target_variable].value_counts()\n",
    "print(\"Label counts in test set:\\n\", label_counts_test)\n",
    "print(\"Label counts in evaluation set:\\n\", label_counts_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['category'].value_counts().plot(kind='bar', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['category'].value_counts().plot(kind='bar', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "eval_dataset = Dataset.from_pandas(df_eval)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length = 512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True, batch_size=len(eval_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything else, we need to verify that we are using the GPU correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_id, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a repository on huggingface and copy its name into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'logs/bert-base-uncased-llm-classificator'\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= output_dir, \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,                \n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',            \n",
    "    logging_dir=f\"{output_dir}/logs\",            \n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\", \n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Compute micro-averaged metrics\n",
    "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    \n",
    "    # Compute macro-averaged metrics\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    \n",
    "\n",
    "    \n",
    "    # Compute accuracy\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "   \n",
    "    metrics = {\n",
    "        'Accuracy': acc,\n",
    "        'Micro_F1': micro_f1,\n",
    "        'Micro_Precision': micro_precision,\n",
    "        'Micro_Recall': micro_recall,\n",
    "        'Macro_F1': macro_f1,\n",
    "        'Macro_Precision': macro_precision,\n",
    "        'Macro_Recall': macro_recall\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=eval_dataset,            \n",
    "    compute_metrics= compute_metrics\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=[trainer.evaluate(eval_dataset=test_dataset)]\n",
    "\n",
    "pd.DataFrame(q, index=[\"test\"]).iloc[:,:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'bert-base-uncased-llm-classificator'\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
