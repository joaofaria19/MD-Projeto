{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch pandas datasets transformers scikit-learn datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "import json\n",
    "from torch import cuda\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    BertForSequenceClassification, \n",
    "    BertTokenizerFast,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset from Hugging Face and using pandas to read the train and test datasets into dataframes, in order to analyse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(file_path, label, category):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "        for e in data:\n",
    "            e['text'] = e.pop('Completion')\n",
    "            e['labels'] = label\n",
    "            e['category'] = category\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_llama = load_and_process_data('meta-llama/Meta-Llama-3-8B-Instruct_valid.json', 0, 'Meta-Llama-3-8B-Instruct')\n",
    "df_phi3 = load_and_process_data('microsoft/Phi-3-mini-4k-instruct_valid.json', 1, 'Phi-3-mini-4k-instruct')\n",
    "df_mixtral = load_and_process_data('mistralai/Mixtral-8x7B-Instruct-v0.1_valid.json', 2, 'Mixtral-8x7B-Instruct-v0.1')\n",
    "df_gpt4 = load_and_process_data('openai/GPT4_valid.json', 3, 'GPT4')\n",
    "\n",
    "df_combined = pd.concat([df_llama, df_phi3, df_mixtral, df_gpt4], ignore_index=True)\n",
    "\n",
    "unique_ids = df_combined['id'].unique()\n",
    "\n",
    "unique_ids_list = unique_ids.tolist()\n",
    "unique_ids_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_combined = df_combined['labels'] \n",
    "\n",
    "\n",
    "label_counts_combined = labels_combined.value_counts()\n",
    "\n",
    "label_counts_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Meta-Llama-3-8B-Instruct','Phi-3-mini-4k-instruct', 'Mixtral-8x7B-Instruct-v0.1', 'GPT4']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = len(labels)\n",
    "\n",
    "id2label={id:label for id,label in enumerate(labels)}\n",
    "label2id={label:id for id,label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_ids, test_eval_ids = train_test_split(unique_ids, train_size=train_size, random_state=42)\n",
    "df_train = df_combined[df_combined['id'].isin(train_ids)]\n",
    "print(\"Train Set Shape:\", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['category'].value_counts().plot(kind='bar', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.5\n",
    "\n",
    "test_ids, eval_ids = train_test_split(test_eval_ids, test_size=test_size, random_state=42)\n",
    "df_test = df_combined[df_combined['id'].isin(test_ids)]\n",
    "df_eval = df_combined[df_combined['id'].isin(eval_ids)]\n",
    "\n",
    "print(\"Test Set Shape:\", df_test.shape)\n",
    "print(\"Evaluation Set Shape:\", df_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['category'].value_counts().plot(kind='bar', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['category'].value_counts().plot(kind='bar', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "eval_dataset = Dataset.from_pandas(df_eval)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length = 512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True, batch_size=len(eval_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything else, we need to verify that we are using the GPU correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_id, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a repository on huggingface and copy its name into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'logs/bert-base-uncased-llm-classificator'\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= output_dir, \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=64,  \n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=100,                \n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',            \n",
    "    logging_dir=f\"{output_dir}/logs\",            \n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\", \n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    \n",
    "    class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(labels, preds, average=None)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': acc,\n",
    "        'Macro_F1': macro_f1,\n",
    "        'Macro_Precision': macro_precision,\n",
    "        'Macro_Recall': macro_recall,\n",
    "        'Class_Precision': class_precision.tolist(),\n",
    "        'Class_Recall': class_recall.tolist(),\n",
    "        'Class_F1': class_f1.tolist()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=eval_dataset,            \n",
    "    compute_metrics= compute_metrics\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"q=[trainer.evaluate(eval_dataset=test_dataset)]\n",
    "\n",
    "pd.DataFrame(q, index=[\"test\"]).iloc[:,:8]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_predicted, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_predicted, normalize='true')\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='.2f', ax=ax, colorbar=False)\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "y_valid = test_dataset['labels']\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_predictions = predictions.predictions.argmax(-1)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_predictions, y_valid, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'bert-base-uncased-llm-classificator'\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
